program: examples/debug_transformer_wandb.py
method: grid
metric:
  goal: maximize
  name: best_probe_accuracy
parameters:
  # Base configuration
  config:
    value: "6block"
  
  # FIXED ARCHITECTURE
  num_blocks:
    value: 6
  
  # MULTIPLE SEEDS for confirmation
  seed:
    values: [10, 20, 30, 40, 50]  # 5 seeds for robust confirmation
  
  # BEST L1/L2 VALUES (update these after initial search)
  # Example - replace with your best values from random search:
  intermediate_l1_coeff:
    values: [0.001, 0.01]  # Replace with 2-3 best values
  
  intermediate_l2_coeff:
    values: [0.001, 0.01]  # Replace with 2-3 best values
  
  # BEST OTHER PARAMETERS (update these after initial search)
  peak_lr_hidden:
    value: 0.095  # Replace with best value
  
  inference_lr_scale_base:
    value: 1.25  # Replace with best value
  
  hidden_momentum:
    value: 0.4  # Replace with best value
  
  h_grad_clip_norm:
    value: 2000.0  # Replace with best value
    
  w_grad_clip_norm:
    value: 500.0  # Replace with best value
  
  inference_steps:
    value: 20  # Replace with best value
  
  # FIXED PARAMETERS
  batch_size:
    value: 200
  hidden_size:
    value: 64
  num_heads:
    value: 1
  warmup_steps:
    value: 0
  use_vode_state_layernorm:
    value: false
  
  # TRAINING SETTINGS (longer for final confirmation)
  epochs:
    value: 25  # Longer for final evaluation
  theta:
    value: 10000
  use_ssl_augmentations:
    value: true
  use_cifar10_norm:
    value: false
  num_images:
    value: 3
  test_subset:
    value: 200  # Larger for final evaluation
  train_subset:
    value: 50000  # Larger for final evaluation
  peak_lr_weights:
    value: 0.001
  hidden_lr_inference:
    value: 0.095
  reconstruction_every_n_epochs:
    value: 10
  validation_every_n_epochs:
    value: 5
  use_inference_lr_scaling:
    value: true
  use_lr_schedule_w:
    value: true
  use_lr_schedule_h:
    value: true
  weight_decay:
    value: 0.0002
  mlp_ratio:
    value: 4.0
  patch_size:
    value: 4
  use_noise:
    value: true
  update_weights_every_inference_step:
    value: false
  use_early_stopping:
    value: true
  early_stopping_patience:
    value: 10
  early_stopping_min_delta:
    value: 0.001
  early_stopping_metric:
    value: "train_mse"
  save_model_train_mse_threshold:
    value: 0.009
  model_saving_metric:
    value: "train_mse"
  use_vode_grad_norm:
    value: false
  use_adamw_for_hidden_optimizer:
    value: false
  corrupt_ratio:
    value: 0.25
  use_lower_half_mask:
    value: false
  inference_clamp_alpha:
    value: 1.0
  save_reconstruction_images:
    value: true
  save_reconstruction_video:
    value: true
  video_fps:
    value: 5
  reinitialize_model_for_each_epoch:
    value: false
  use_status_init_in_training:
    value: false
  use_status_init_in_unmasking:
    value: false
  lr_schedule_min_lr_factor:
    value: 0.5
  linear_probe_every_n_epochs:
    value: 5
  linear_probe_vode_indices:
    value: "0"
  linear_probe_concatenate_features:
    value: true
  linear_probe_use_gap:
    value: true
  linear_probe_lr:
    value: 0.001
  linear_probe_wd:
    value: 0.0001
  linear_probe_epochs:
    value: 20  # More epochs for final evaluation
  linear_probe_batch_size:
    value: 200
  linear_probe_seed:
    value: 123 