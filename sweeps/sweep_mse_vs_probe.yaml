program: examples/debug_transformer_wandb.py
method: random
run_cap: 500  # Focused but comprehensive search
metric:
  goal: maximize
  name: best_probe_accuracy
parameters:
  # Base configuration (your proven setup)
  config:
    value: "6block"
  
  # FIXED ARCHITECTURE (your optimized settings)
  num_blocks:
    value: 6
  hidden_size:
    value: 64
  num_heads:
    value: 1
  
  # MULTIPLE SEEDS for robust comparison
  seed:
    values: [10]
  
  # THE KEY EXPLORATION: L1/L2 regularization trade-off
  intermediate_l1_coeff:
    values: [0.0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]  # Include 0.0 + log-spaced values
  
  intermediate_l2_coeff:
    values: [0.0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]  # Include 0.0 + log-spaced values
  
  # VARY LEARNING RATES around your optimal (small perturbations)
  peak_lr_hidden:
    distribution: log_uniform_values
    min: 0.05    # Lower bound around your optimized value
    max: 0.15    # Upper bound around your optimized value
  
  peak_lr_weights:
    distribution: log_uniform_values
    min: 0.0005  # Around your optimized value  
    max: 0.002
  
  # EXPLORE REGULARIZATION TIMING
  # When to start applying regularization
  # (This could be a new parameter to add to your model)
  
  # AUGMENTATION STRENGTH SEARCH
  use_ssl_augmentations:
    values: [false]  # Compare with/without
  
  use_cifar10_norm:
    values: [true, false]  # Different normalization strategies
  
  # CORRUPTION STRATEGIES for different representations
  corrupt_ratio:
    values: [0.25]  # Different masking ratios
  
  use_lower_half_mask:
    values: [false]  # Structured vs random masking
  
  # OPTIMIZATION VARIATIONS (small perturbations around your optimum)
  inference_lr_scale_base:
    distribution: uniform
    min: 1.1     # Around your optimized value
    max: 1.4

  hidden_momentum:
    distribution: log_uniform_values
    min: 0.1 
    max: 0.95
  
  h_grad_clip_norm:
    distribution: uniform
    min: 1500.0  # Around your optimized value
    max: 2500.0
    
  w_grad_clip_norm:
    distribution: uniform
    min: 400.0   # Around your optimized value
    max: 600.0
  
  inference_steps:
    values: [15, 20, 25]  # Around your optimized value
  
  # TRAINING DYNAMICS EXPLORATION
  use_lr_schedule_h:
    values: [true, false]
  
  use_lr_schedule_w:  
    values: [true, false]
  
  warmup_steps:
    values: [0, 25, 50]  # Light exploration
  
  # PROBE-SPECIFIC OPTIMIZATIONS
  linear_probe_every_n_epochs:
    value: 5  # Frequent evaluation
  
  linear_probe_epochs:
    values: [15]  # Better probe training
  
  linear_probe_lr:
    values: [0.001]  # Probe LR optimization
  
  linear_probe_wd:
    values: [0.0001]  # Probe weight decay
  
  # LONGER TRAINING for better convergence
  epochs:
    value: 30  # Longer to see regularization effects
  
  early_stopping_patience:
    value: 12  # More patience for regularization to take effect
  
  # EVALUATION SETTINGS
  num_images:
    value: 3
  test_subset:
    value: 1000  # Larger for better probe accuracy measurement
  train_subset:
    value: 50000  # Full dataset
  
  # FIXED OPTIMAL SETTINGS (from your month of tuning)
  batch_size:
    value: 200
  patch_size:
    value: 4
  mlp_ratio:
    value: 4.0
  theta:
    value: 10000
  hidden_lr_inference:
    value: 0.095
  reconstruction_every_n_epochs:
    value: 10
  validation_every_n_epochs:
    value: 5
  use_inference_lr_scaling:
    value: true
  weight_decay:
    value: 0.0002
  use_noise:
    value: true
  update_weights_every_inference_step:
    value: false
  use_early_stopping:
    value: true
  early_stopping_min_delta:
    value: 0.001
  early_stopping_metric:
    value: "train_mse"  # Keep MSE for consistency, but optimize for probe
  save_model_train_mse_threshold:
    value: 0.005
  model_saving_metric:
    value: "train_mse"
  use_vode_grad_norm:
    value: false
  use_vode_state_layernorm:
    value: false
  use_adamw_for_hidden_optimizer:
    value: false
  inference_clamp_alpha:
    value: 1.0
  save_reconstruction_images:
    value: true
  save_reconstruction_video:
    value: false
  video_fps:
    value: 5
  reinitialize_model_for_each_epoch:
    value: false
  use_status_init_in_training:
    value: false
  use_status_init_in_unmasking:
    value: false
  lr_schedule_min_lr_factor:
    value: 0.5
  linear_probe_vode_indices:
    value: "0"
  linear_probe_concatenate_features:
    value: true
  linear_probe_use_gap:
    value: true
  linear_probe_batch_size:
    value: 200
  linear_probe_seed:
    value: 123 