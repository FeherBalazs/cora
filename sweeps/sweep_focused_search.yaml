program: examples/debug_transformer_wandb.py
method: bayes
metric:
  goal: maximize
  name: best_probe_accuracy # Make sure your script logs this to W&B

parameters:
  # --- Config and Architectural (Fixed based on hyperparam_search.py) ---
  config:
    value: "6block"
  num_blocks:
    value: 6
  batch_size:
    value: 200
  hidden_size:
    value: 64
  num_heads:
    value: 1

  # --- Hyperparameters to Search (Centered around hyperparam_search.py values) ---
  peak_lr_hidden:
    distribution: uniform
    min: 0.085
    max: 0.105 # Centered around 0.095
  inference_lr_scale_base:
    distribution: uniform
    min: 1.20
    max: 1.30 # Centered around 1.25
  hidden_momentum:
    distribution: uniform
    min: 0.35
    max: 0.45 # Centered around 0.4
  h_grad_clip_norm:
    distribution: q_uniform # For integer-like steps
    min: 1500
    max: 2500
    q: 500 # Steps of 500, e.g., 1500, 2000, 2500. Centered around 2000
  intermediate_l1_coeff:
    distribution: log_uniform_values
    min: 0.00005
    max: 0.0002 # Centered around 0.0001
  intermediate_l2_coeff:
    distribution: uniform # L2 was 0, so explore small positive values
    min: 0.0
    max: 0.0001

  # --- Fixed Hyperparameters (from hyperparam_search.py's single-value candidates) ---
  seed:
    value: 10 # Fixed seed for Bayesian optimizer consistency
  inference_steps:
    value: 20
  warmup_steps:
    value: 0
  w_grad_clip_norm:
    value: 500.0
  use_vode_state_layernorm:
    value: false

  # --- Fixed Overrides (from hyperparam_search.py's fixed_overrides dict) ---
  epochs:
    value: 75
  theta:
    value: 10000
  use_ssl_augmentations:
    value: false # As requested, turn off for this sweep
  use_cifar10_norm:
    value: true # Journal indicated this helped
  num_images:
    value: 3
  test_subset:
    value: 200
  train_subset:
    value: 50000 # Full CIFAR-10
  peak_lr_weights:
    value: 0.001
  hidden_lr_inference:
    value: 0.095 # Matches initial peak_lr_hidden center
  reconstruction_every_n_epochs:
    value: 25
  validation_every_n_epochs:
    value: 25
  use_inference_lr_scaling:
    value: true
  use_lr_schedule_w:
    value: true
  use_lr_schedule_h:
    value: true
  weight_decay:
    value: 0.0002
  mlp_ratio:
    value: 4.0
  patch_size:
    value: 4
  use_noise:
    value: true # This was True in hyperparam_search.py
  update_weights_every_inference_step:
    value: false
  use_early_stopping:
    value: true
  early_stopping_patience:
    value: 20 # Reduced for sweep efficiency
  early_stopping_min_delta:
    value: 0.001
  early_stopping_metric:
    value: "train_mse" # Or "val_mse" if you prefer, ensure it's logged
  save_model_train_mse_threshold:
    value: 0.008 # Keep or adjust based on expected MSE without augs
  model_saving_metric:
    value: "train_mse" # Or "val_mse"
  use_vode_grad_norm:
    value: false
  use_adamw_for_hidden_optimizer:
    value: false
  corrupt_ratio:
    value: 0.25 # Not used if use_ssl_augmentations is false for corruption-based augs
  use_lower_half_mask:
    value: false # Not used if use_ssl_augmentations is false for masking
  inference_clamp_alpha:
    value: 1.0
  save_reconstruction_images:
    value: true
  save_reconstruction_video:
    value: true
  video_fps:
    value: 5
  reinitialize_model_for_each_epoch:
    value: false
  use_status_init_in_training:
    value: false
  use_status_init_in_unmasking:
    value: false
  lr_schedule_min_lr_factor:
    value: 0.5

  # --- Linear Probing Settings (adapted for sweep) ---
  linear_probe_every_n_epochs:
    value: 25 # Probe at epoch 25, 50, 75
  linear_probe_vode_indices:
    value: "0,1,4,7" # Your best combination
  linear_probe_concatenate_features:
    value: true
  linear_probe_use_gap:
    value: true
  linear_probe_lr:
    value: 0.001
  linear_probe_wd:
    value: 0.0001
  linear_probe_epochs:
    value: 50 # Reduced for faster sweep runs
  linear_probe_batch_size:
    value: 200
  linear_probe_seed:
    value: 123 